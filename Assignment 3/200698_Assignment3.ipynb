{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbtPCeppZe-F"
      },
      "source": [
        "# Value Iteration Q-Learning Agent to play the Frozen Lake Game\n",
        "\n",
        "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:*\n",
        "\n",
        "<center> SFFF </center> \n",
        "<center>FHFH </center>\n",
        "<center>FFFH </center>\n",
        "<center>HFFG </center>\n",
        "\n",
        "This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
        "\n",
        "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n",
        "\n",
        "- S: Starting point, Safe. 0 Reward \\\\\n",
        "- F: Frozen Lake, Safe. 0 Reward \\\\\n",
        "- H: Hole, Unsafe. Episode Ends. 0 Reward \\\\\n",
        "- G: Goal. Episode ends. 1 Reward \\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTOJgJQQOIiS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5lzbwRhDQ4"
      },
      "source": [
        "## Creating the environment\n",
        "Using open AI gym, we can load the environment of the Frozen Lake and render it using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ__qCIWPhv1",
        "outputId": "e4ecd778-7093-40c6-d3c9-45df15e05cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxXj6yCg4OW"
      },
      "source": [
        "## Constructing the Q Table\n",
        "We're now going to construct our Q-table, and initialize all the Q-values to zero for each state-action pair.\n",
        "\n",
        "Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using env.observation_space.n and env.action_space.n, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBSTDCZPqVC",
        "outputId": "511a2d33-abc0-4046-ed32-33fac562a295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size)) #Initialize q_table to a np array of zeros of shape state_space_size x action_space_size\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AiUoFW4hrBz"
      },
      "source": [
        "## Initialize Q-Learning Parameters\n",
        "\n",
        "Initialize the parameters for the q learning agent. Challenge: Change these values to get better results than the ones with these original values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_tB2e-9Pxgc"
      },
      "outputs": [],
      "source": [
        "num_episodes = 10000 \n",
        "max_steps_per_episode = 100 #Maximum number of moves per episode\n",
        "\n",
        "learning_rate = 0.05 #Learning rate for updating the q values.\n",
        "discount_rate = 0.99\n",
        "\n",
        "exploration_rate = 1 #Initial exploration rate\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfh8GteKiI0g"
      },
      "source": [
        "## Train the Q-Learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ47-6QVP0y1"
      },
      "outputs": [],
      "source": [
        "rewards_all_episodes = [] #This will store the rewards in each episode\n",
        "for episode in range(num_episodes): #Iterate over each episode\n",
        "    state = env.reset() #Reset the environment before starting each episode\n",
        "    done = False #The done variable tells if the episode is over yet\n",
        "    rewards_current_episode = 0\n",
        "    for step in range(max_steps_per_episode): #Iterate over the steps\n",
        "\n",
        "    ##### Exploration-exploitation trade-off #####\n",
        "        #Generate a random sample between 0 and 1, explore if this value is less than the exploration rate, exploit otherwise\n",
        "        exploration_rate_threshold = random.uniform(0,1) \n",
        "        if exploration_rate_threshold > exploration_rate:\n",
        "            action = np.argmax(q_table[state,:]) \n",
        "        else:\n",
        "            action = env.action_space.sample() #Hint: You want your action to be randomly chosen from {0,1,2,3} in this case\n",
        "            \n",
        "        new_state, reward, done, info = env.step(action) #Get the updated environment\n",
        "        \n",
        "        #Update the q table using the value iteration formula\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "        state = new_state\n",
        "        rewards_current_episode += reward \n",
        "        if done == True: \n",
        "            break\n",
        "          \n",
        "    #Update the exploration rate        \n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) \n",
        "    rewards_all_episodes.append(rewards_current_episode) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bokzQMSzkbQe"
      },
      "source": [
        "Look at the values of the reward per thousand episodes. It should be increasing if your agent is learning properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADArx_TMSIwQ",
        "outputId": "1a5baa5e-a36e-498d-b975-9cfcd7f578b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.04300000000000003\n",
            "2000 :  0.19800000000000015\n",
            "3000 :  0.4070000000000003\n",
            "4000 :  0.5740000000000004\n",
            "5000 :  0.6180000000000004\n",
            "6000 :  0.6690000000000005\n",
            "7000 :  0.6980000000000005\n",
            "8000 :  0.7040000000000005\n",
            "9000 :  0.6750000000000005\n",
            "10000 :  0.7160000000000005\n"
          ]
        }
      ],
      "source": [
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"********Average reward per thousand episodes********\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRdL9-Dykqe4",
        "outputId": "1a5a79e4-18c5-49d1-d2c6-aea72ed2b388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.57167596 0.54216396 0.53900705 0.5338552 ]\n",
            " [0.27621282 0.32140462 0.30082611 0.52869067]\n",
            " [0.44235584 0.40940537 0.40430461 0.49758144]\n",
            " [0.3211564  0.36309904 0.25882046 0.47944005]\n",
            " [0.58817814 0.39342404 0.3525503  0.36208995]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21092903 0.2160294  0.39917689 0.16138619]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.41976919 0.39802194 0.41382696 0.62645387]\n",
            " [0.35390398 0.67461136 0.43783381 0.33740017]\n",
            " [0.60303274 0.42049938 0.43218607 0.30058745]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.39649383 0.56792192 0.78893972 0.5812407 ]\n",
            " [0.72221288 0.90018348 0.79199654 0.77800173]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "#Final Q Table\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtppFo9kkuxD"
      },
      "source": [
        "## Watch the trained agent play Frozen Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX7pM43dSP-x",
        "outputId": "07a5f3fb-d0eb-425c-99c5-cd0152880351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "****You reached the goal!****\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for episode in range(3):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "    for step in range(max_steps_per_episode):        \n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "        time.sleep(0.3)\n",
        "        action = np.argmax(q_table[state,:])   #Since this is not part of training, we want the agent to exploit all the time.     \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            clear_output(wait=True)\n",
        "            env.render()\n",
        "            if reward == 1:\n",
        "                print(\"****You reached the goal!****\")\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                print(\"****You fell through a hole!****\")\n",
        "                time.sleep(3)\n",
        "                clear_output(wait=True)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "200698_Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0f7ff968abd5cdc646c3b0d79d8d9fba0fbc1faf12e0395fe0c5d95a57fad77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
